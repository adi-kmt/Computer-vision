{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbzJRKOb5cdN"
      },
      "source": [
        "Inspiration from the DCGAN pytorch tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-gmZVZkr7hv"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ6MTcC_5RjI"
      },
      "source": [
        "##Dataset\n",
        "Using the Anime kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYRtqm5rHJIz"
      },
      "source": [
        "train_dir='./animefacedataset'\n",
        "print(os.listdir(train_dir+'/images')[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UUwYN1iWyTy"
      },
      "source": [
        "##Hyperparameters\n",
        "Keeping important hyperparameters, may also be stored in a config file using hydra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxsZ5zLcHm_z"
      },
      "source": [
        "#Important parameters according to the paper\n",
        "lr=0.0002 #learning rate\n",
        "batch_size=128 #batch size\n",
        "beta_1=0.5 #momentum beta1\n",
        "beta_2=0.999 #momentum beta2\n",
        "slope=0.2 #Leaky ReLU\n",
        "num_epochs=30 #Number of epochs\n",
        "image_size=64 #Image size of inputs\n",
        "random_seed=35 #Seed for random generation for reproducibility\n",
        "n=30080 #Number of pictures to be takes\n",
        "device=torch.device('cuda') #Using CUDA device\n",
        "noise=100 #Noise dimension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rqtZieoW9Wx"
      },
      "source": [
        "##Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS7ZGWZ4Ty8q"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQzcjj0nUEj2"
      },
      "source": [
        "transforms=T.Compose([\n",
        "                      T.ToTensor(),\n",
        "                      T.Resize((image_size, image_size)),\n",
        "                      T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Bringing images to (-1,1) \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ady9a68XV4S"
      },
      "source": [
        "Since about 60k+ images are present, only 30k+ images are taken to reduce training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MpuciZFaMhm"
      },
      "source": [
        "np.random.seed(random_seed)\n",
        "data = ImageFolder(train_dir, transform=transforms)\n",
        "train_data=Subset(data, np.random.choice(len(data), n, replace=False))\n",
        "train = DataLoader(train_data, batch_size, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViBgUKoaVvOS"
      },
      "source": [
        "##Model  for DCGAN\n",
        "First Generator:\n",
        "1. Transpose Conv2D\n",
        "2. BatchNorm \n",
        "3. ReLU, (but Tanh for the last layer to convert image to (-1,1) )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLaBVF53HskF"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOhlxt8AINb0"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.main=nn.Sequential(\n",
        "        self.gen_layer(noise,512,4,1,0),\n",
        "        self.gen_layer(512,256,4,2,1),\n",
        "        self.gen_layer(256,128,4,2,1),\n",
        "        self.gen_layer(128,64,4,2,1),\n",
        "        nn.ConvTranspose2d(in_channels=64, out_channels=3,\n",
        "                             kernel_size=4, stride=2, padding=1),\n",
        "        nn.Tanh())\n",
        "\n",
        "  def gen_layer(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(False))\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return self.main(x)\n",
        "\n",
        "Generator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2_rhosXWOZt"
      },
      "source": [
        "Then Discriminator:\n",
        "1. Conv2D\n",
        "2. BatchNorm (only for the middle layers)\n",
        "3. Leaky ReLU (but Sigmoid in the last layer for class probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i40Q4BaCNBfA"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.main=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=64,\n",
        "                             kernel_size=4, stride=2, padding=1),\n",
        "        nn.LeakyReLU(slope),\n",
        "        self.disc_block(64,128,4,2,1),\n",
        "        self.disc_block(128,256,4,2,1),\n",
        "        self.disc_block(256,512,4,2,1),\n",
        "        nn.Conv2d(in_channels=512, out_channels=1,\n",
        "                             kernel_size=4, stride=1, padding=0),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def disc_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "          nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.LeakyReLU(slope)) #taking the slope from the previous set values\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.main(x)\n",
        "\n",
        "Discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwymTNWWjB4"
      },
      "source": [
        "Initiating weights for Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GELgxc2JY0F"
      },
      "source": [
        "def initialise_weights(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "      nn.init.normal_(m.weight.data, 0, 0.2)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "      nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "      nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "netG=Generator().apply(initialise_weights)\n",
        "netD=Discriminator().apply(initialise_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H36NuZpWpnl"
      },
      "source": [
        "Choosing other important metrics such as BCELoss and Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0JC3pPKZG_"
      },
      "source": [
        "criterion=nn.BCELoss()\n",
        "fixed_random_noise=torch.randn(batch_size, noise, 1,1)\n",
        "optim_D=optim.Adam(netD.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "optim_G=optim.Adam(netG.parameters(), lr=lr, betas=(beta_1, beta_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2LA1t-BXKVD"
      },
      "source": [
        "##Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQKbhU7kPp35"
      },
      "source": [
        "netG.to(device)\n",
        "netD.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "  for idx, (img,_) in enumerate(train):\n",
        "    netG.train()\n",
        "    netD.train()\n",
        "    \n",
        "    #Training Discriminant maximise loss= log D(x) + log (1-D(G(z))) \n",
        "    #Passing Real images to discriminant\n",
        "    img=img.to(device)\n",
        "    real_out=netD(img).view(-1)\n",
        "    real_labels=torch.full_like(real_out, 0.95).to(device) #Instead of using 1, we can use 0.95 to improve the training\n",
        "    loss_real=criterion(real_out, real_labels.detach())\n",
        "    \n",
        "    #Passing Fake images to the Discriminator, after passing through the generator\n",
        "    fixed_random_noise=fixed_random_noise.to(device)\n",
        "    fake_imgs=netG(fixed_random_noise) #Generate the fake images\n",
        "    fake_out=netD(fake_imgs.detach()).view(-1)\n",
        "    fake_labels=torch.full_like(fake_out, 0.05).to(device) #Instead of using 0, we can use 0.05 to improve the training\n",
        "    loss_fake=criterion(fake_out, fake_labels.detach())\n",
        "\n",
        "    loss_d=(loss_real + loss_fake)/2\n",
        "\n",
        "    netD.zero_grad()\n",
        "    loss_d.backward()\n",
        "    optim_D.step()\n",
        "\n",
        "    #Training Generator maximise loss=log D(G(z))\n",
        "    fake_img_gen=netD(fake_imgs).view(-1)\n",
        "    make_it_real=torch.ones_like(fake_img_gen).to(device)\n",
        "    gen_loss=criterion(fake_img_gen, make_it_real.detach())\n",
        "\n",
        "    netG.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    optim_G.step()\n",
        "\n",
        "\n",
        "  print('Epoch',epoch+1)\n",
        "\n",
        "torch.save(netG.state_dict(), 'G.pth')\n",
        "torch.save(netD.state_dict(), 'D.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "svbzQ4uR20v8"
      },
      "source": [
        "with torch.no_grad():\n",
        "  plt.figure(figsize=(8,8))\n",
        "  fake=netG(fixed_random_noise).cpu()\n",
        "  plt.imshow(np.transpose(make_grid(fake[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}